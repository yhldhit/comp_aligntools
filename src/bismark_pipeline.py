
import os
import logging
import multiprocessing
from itertools import product
import argparse
import time
import sys
import re # Import re for flexible fastq file matching
import glob # To find fastq files

# --- Main Processing Pipeline Function ---
def process_cell_pipeline(output_dir, cell_id, fastq_dir, args):
    """
    Processes a single cell from FASTQ through ALLC generation, mimicking the Snakefile.
    Assumes input FASTQ files are named {cell_id}_1.fastq.gz and {cell_id}_2.fastq.gz.
    """
    print(f"--- Starting processing for cell: {cell_id} ---", flush=True)
    start_time_cell = time.time()

    # Define subdirectories within the main output directory
    fastq_out_dir = os.path.join(output_dir, 'fastq') # For trimmed fastq
    bam_out_dir = os.path.join(output_dir, 'bam')
    allc_out_dir = os.path.join(output_dir, 'allc')
    mcg_context_dir = os.path.join(output_dir, f"allc-{args.mcg_context}")
    temp_dir = os.path.join(bam_out_dir, 'temp') # For Picard

    # Create directories if they don't exist
    os.makedirs(fastq_out_dir, exist_ok=True)
    os.makedirs(bam_out_dir, exist_ok=True)
    os.makedirs(allc_out_dir, exist_ok=True)
    os.makedirs(mcg_context_dir, exist_ok=True)
    os.makedirs(temp_dir, exist_ok=True)

    # --- Define File Paths ---
    # Input FastQ (using the provided fastq_dir and cell_id)
    r1_fq_in = os.path.join(fastq_dir, f"{cell_id}_1.fastq.gz")
    r2_fq_in = os.path.join(fastq_dir, f"{cell_id}_2.fastq.gz")

    # Check if input fastq files exist
    if not os.path.exists(r1_fq_in):
        print(f"ERROR: Input R1 FASTQ file not found: {r1_fq_in}. Skipping cell {cell_id}.", flush=True)
        return
    if not os.path.exists(r2_fq_in):
        print(f"ERROR: Input R2 FASTQ file not found: {r2_fq_in}. Skipping cell {cell_id}.", flush=True)
        return

    # Trimmed FastQ (intermediate, placed in fastq_out_dir)
    # Using Snakefile naming convention for intermediates
    r1_trimmed_fq = os.path.join(fastq_out_dir, f"{cell_id}-R1.trimmed.fq.gz")
    r2_trimmed_fq = os.path.join(fastq_out_dir, f"{cell_id}-R2.trimmed.fq.gz")
    r1_trimmed_stats = os.path.join(fastq_out_dir, f"{cell_id}-R1.trimmed.stats.tsv")
    r2_trimmed_stats = os.path.join(fastq_out_dir, f"{cell_id}-R2.trimmed.stats.tsv")

    # Bismark output (intermediate, placed in bam_out_dir)
    r1_bismark_bam = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2.bam")
    r2_bismark_bam = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2.bam")
    r1_bismark_report = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2_SE_report.txt")
    r2_bismark_report = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2_SE_report.txt")

    # Filtered BAM (intermediate)
    r1_filtered_bam = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2.filter.bam")
    r2_filtered_bam = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2.filter.bam")

    # Sorted BAM (intermediate)
    r1_sorted_bam = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2.sorted.bam")
    r2_sorted_bam = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2.sorted.bam")

    # Deduplicated BAM (intermediate)
    r1_dedup_bam = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2.deduped.bam")
    r2_dedup_bam = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2.deduped.bam")
    r1_dedup_matrix = os.path.join(bam_out_dir, f"{cell_id}-R1.trimmed_bismark_bt2.deduped.matrix.txt")
    r2_dedup_matrix = os.path.join(bam_out_dir, f"{cell_id}-R2.trimmed_bismark_bt2.deduped.matrix.txt")

    # Merged Final BAM
    final_bam = os.path.join(bam_out_dir, f"{cell_id}.final.bam")

    # ALLC files
    allc_file = os.path.join(allc_out_dir, f"{cell_id}.allc.tsv.gz")
    allc_count_file = os.path.join(allc_out_dir, f"{cell_id}.allc.tsv.gz.count.csv") # As generated by allcools
    # Final target file to check for completion
    mcg_context_allc_file = os.path.join(mcg_context_dir, f"{cell_id}.{args.mcg_context}-Merge.allc.tsv.gz")
    mcg_context_prefix = os.path.join(mcg_context_dir, f"{cell_id}") # Prefix for extract-allc

    # --- Check if final output already exists ---
    if os.path.exists(mcg_context_allc_file) and not args.force_rerun:
        print(f"INFO: Final output {mcg_context_allc_file} already exists for cell {cell_id}. Skipping.", flush=True)
        return

    # --- Commands ---
    # 1. Trim reads (Cutadapt R1)
    # Input is the original R1 fastq, output is the trimmed R1 fastq in fastq_out_dir
    cmd_trim_r1_pass1 = f"cutadapt --report=minimal -a {args.r1_adapter} {r1_fq_in} 2> {r1_trimmed_stats}"
    cmd_trim_r1_pass2 = f"cutadapt --report=minimal -O 6 -q 20 -u {args.r1_left_cut} -u -{args.r1_right_cut} -m 30 -o {r1_trimmed_fq} - >> {r1_trimmed_stats}"
    cmd_trim_r1 = f"{cmd_trim_r1_pass1} | {cmd_trim_r1_pass2}"

    # 2. Trim reads (Cutadapt R2)
    cmd_trim_r2_pass1 = f"cutadapt --report=minimal -a {args.r2_adapter} {r2_fq_in} 2> {r2_trimmed_stats}"
    cmd_trim_r2_pass2 = f"cutadapt --report=minimal -O 6 -q 20 -u {args.r2_left_cut} -u -{args.r2_right_cut} -m 30 -o {r2_trimmed_fq} - >> {r2_trimmed_stats}"
    cmd_trim_r2 = f"{cmd_trim_r2_pass1} | {cmd_trim_r2_pass2}"

    # 3. Bismark R1 (--pbat) using trimmed R1 fastq
    cmd_bismark_r1 = (f"bismark {args.bismark_reference} --bowtie2 {r1_trimmed_fq} "
                    f"--pbat -p 10 -o {bam_out_dir}/ --temp_dir {bam_out_dir}/")

    # 4. Bismark R2 (normal SE) using trimmed R2 fastq
    cmd_bismark_r2 = (f"bismark {args.bismark_reference} -p 10 --bowtie2 {r2_trimmed_fq} "
                    f"-o {bam_out_dir}/ --temp_dir {bam_out_dir}/")

    # 5. Filter R1 BAM (MAPQ >= 10)
    cmd_filter_r1 = f"samtools view -b -h -q 10 -o {r1_filtered_bam} {r1_bismark_bam}"

    # 6. Filter R2 BAM (MAPQ >= 10)
    cmd_filter_r2 = f"samtools view -b -h -q 10 -o {r2_filtered_bam} {r2_bismark_bam}"

    # 7. Sort R1 BAM
    cmd_sort_r1 = f"samtools sort -o {r1_sorted_bam} {r1_filtered_bam}"

    # 8. Sort R2 BAM
    cmd_sort_r2 = f"samtools sort -o {r2_sorted_bam} {r2_filtered_bam}"

    # 9. Deduplicate R1 BAM (Picard)
    cmd_dedup_r1 = (f"java -jar /home/wang_yanni/anaconda3/envs/py38/share/picard-3.4.0-0/picard.jar  MarkDuplicates I={r1_sorted_bam} O={r1_dedup_bam} M={r1_dedup_matrix} "
                    f"REMOVE_DUPLICATES=true TMP_DIR={temp_dir}/")

    # 10. Deduplicate R2 BAM (Picard)
    cmd_dedup_r2 = (f"java -jar /home/wang_yanni/anaconda3/envs/py38/share/picard-3.4.0-0/picard.jar MarkDuplicates I={r2_sorted_bam} O={r2_dedup_bam} M={r2_dedup_matrix} "
                    f"REMOVE_DUPLICATES=true TMP_DIR={temp_dir}/")

    # 11. Merge R1 and R2 deduplicated BAMs
    cmd_merge_bam = f"samtools merge -f {final_bam} {r1_dedup_bam} {r2_dedup_bam}"

    # 14. Cleanup Intermediate Files
    intermediate_files = [
        r1_trimmed_fq, r2_trimmed_fq, # Trimmed fastq
        r1_trimmed_stats, r2_trimmed_stats, # Trim stats
        r1_bismark_bam, r2_bismark_bam, # Original bismark output
        r1_bismark_report, r2_bismark_report, # Bismark reports
        r1_filtered_bam, r2_filtered_bam, # Filtered bam
        r1_sorted_bam, r2_sorted_bam, # Sorted bam
        r1_dedup_bam, r2_dedup_bam, # Deduped bam before merge
        r1_dedup_matrix, r2_dedup_matrix, # Picard metrics
        final_bam, # Final merged bam
    ]
    # Construct rm command safely
    # files_to_remove_str = " ".join([f"'{f}'" for f in intermediate_files if os.path.exists(f)]) # Quote filenames
    # cmd_cleanup = f"rm -f {files_to_remove_str}" if files_to_remove_str else "echo 'No intermediate files to remove'"


    # --- Execute Pipeline Steps ---
    commands = [
        ("Trim R1", cmd_trim_r1),
        ("Trim R2", cmd_trim_r2),
        ("Bismark R1", cmd_bismark_r1),
        ("Bismark R2", cmd_bismark_r2),
        ("Filter R1", cmd_filter_r1),
        ("Filter R2", cmd_filter_r2),
        ("Sort R1", cmd_sort_r1),
        ("Sort R2", cmd_sort_r2),
        ("Deduplicate R1", cmd_dedup_r1),
        ("Deduplicate R2", cmd_dedup_r2),
        ("Merge BAMs", cmd_merge_bam),
        ]

    for name, cmd in commands:
        print(f"  Executing Step: {name} for {cell_id}", flush=True)
        # print(f"    Command: {cmd}", flush=True) # Uncomment for debugging
        step_start_time = time.time()
        exit_code = os.system(cmd)
        step_end_time = time.time()
        duration = step_end_time - step_start_time
        if exit_code != 0:
            print(f"  ERROR: Step '{name}' failed for cell {cell_id} with exit code {exit_code}. Stopping pipeline for this cell.", flush=True)
            # Attempt to remove potentially corrupted final output if step failed
            if os.path.exists(mcg_context_allc_file):
                print(f"  Attempting to remove potentially incomplete output: {mcg_context_allc_file}", flush=True)
                os.remove(mcg_context_allc_file)
            return # Stop processing this cell
        else:
            print(f"  Step '{name}' completed successfully for {cell_id} in {duration:.2f} seconds.", flush=True)
        sys.stdout.flush() # Ensure output is flushed

    end_time_cell = time.time()
    print(f"--- Finished processing cell {cell_id}. Total time: {end_time_cell - start_time_cell:.2f} seconds ---", flush=True)


def helper_process_cell_pipeline(args_tuple):
    # Unpack arguments for the worker function
    output_dir, cell_id, fastq_dir, cli_args = args_tuple
    try:
        process_cell_pipeline(output_dir, cell_id, fastq_dir, cli_args)
    except Exception as e:
        print(f"FATAL ERROR processing cell {cell_id}: {e}", flush=True)
        # Optionally re-raise or log more details
        # raise e


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Process Bisulfite sequencing data from paired-end FASTQ to ALLC, mimicking mc.Snakefile.')

    # Input/Output
    parser.add_argument('-i', '--input_dir', dest='input_dir', type=str, required=True,
                        help='Directory containing input paired-end FASTQ files (named SRRxxxxxx_1.fastq.gz and SRRxxxxxx_2.fastq.gz).')
    parser.add_argument('-o', '--output_dir', dest='output_dir', type=str, required=True,
                        help='Main output directory where subdirectories (fastq, bam, allc) will be created.')
    parser.add_argument('-p', '--processes', dest='processes', type=int, default=1,
                        help='Number of cells (SRR IDs) to process in parallel.')
    parser.add_argument('--force_rerun', action='store_true', help='Force reprocessing even if final output exists.')


    # Cutadapt parameters
    parser.add_argument('--r1_adapter', type=str, default='AGATCGGAAGAGCACACGTCTGAACTCCAGTCA', help='Adapter sequence for R1.')
    parser.add_argument('--r2_adapter', type=str, default='AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT', help='Adapter sequence for R2.')
    parser.add_argument('--r1_left_cut', type=int, default=0, help='Number of bases to cut from the 5\' end of R1 after adapter trimming.')
    parser.add_argument('--r1_right_cut', type=int, default=0, help='Number of bases to cut from the 3\' end of R1 after adapter trimming.')
    parser.add_argument('--r2_left_cut', type=int, default=0, help='Number of bases to cut from the 5\' end of R2 after adapter trimming.')
    parser.add_argument('--r2_right_cut', type=int, default=0, help='Number of bases to cut from the 3\' end of R2 after adapter trimming.')

    # Bismark parameters
    parser.add_argument('--bismark_reference', type=str, required=True, help='Path to the Bismark reference genome index.')
    # parser.add_argument('--unmapped_param_str', type=str, default='', help='Optional parameters for unmapped reads in Bismark (e.g., --unmapped).') # Not used currently

    # Picard parameters (implicitly uses defaults, path might be needed if not in PATH)
    # parser.add_argument('--picard_jar', type=str, required=True, help='Path to the picard.jar file.') # Add if picard isn't in PATH

    # ALLCools parameters
    parser.add_argument('--reference_fasta', type=str, required=True, help='Path to the reference genome FASTA file.')
    parser.add_argument('--num_upstr_bases', type=int, default=0, help='Number of upstream bases to include in ALLC context.')
    parser.add_argument('--num_downstr_bases', type=int, default=2, help='Number of downstream bases to include in ALLC context.')
    parser.add_argument('--compress_level', type=int, default=5, help='Compression level for ALLC file.')
    parser.add_argument('--mcg_context', type=str, default='CGN', help='Methylation context to extract (e.g., CGN, CHN). Use HCGN for NOMe-seq if num_upstr_bases=0.')
    parser.add_argument('--chrom_size_path', type=str, required=True, help='Path to the chromosome sizes file (tab-separated: chromName size).')


    args = parser.parse_args()

    # --- Discover FASTQ files and determine Cell IDs (SRR IDs) ---
    fastq_files_r1 = glob.glob(os.path.join(args.input_dir, '*_1.fastq.gz'))
    cell_ids_to_process = set()

    if not fastq_files_r1:
        print(f"ERROR: No '*_1.fastq.gz' files found in {args.input_dir}", flush=True)
        sys.exit(1)

    for r1_file in fastq_files_r1:
        basename = os.path.basename(r1_file)
        match = re.match(r'(.+)_1\.fastq\.gz', basename)
        if match:
            cell_id = match.group(1)
            # Check if the corresponding R2 file exists
            r2_file = os.path.join(args.input_dir, f"{cell_id}_2.fastq.gz")
            if os.path.exists(r2_file):
                cell_ids_to_process.add(cell_id)
            else:
                print(f"WARN: Found {r1_file} but corresponding {r2_file} is missing. Skipping {cell_id}.", flush=True)
        else:
            print(f"WARN: Could not parse cell ID from {basename}. Skipping.", flush=True)

    if not cell_ids_to_process:
        print(f"ERROR: No valid paired FASTQ files found to process in {args.input_dir}", flush=True)
        sys.exit(1)

    print(f"Found {len(cell_ids_to_process)} cell IDs (SRR IDs) to process: {', '.join(sorted(list(cell_ids_to_process)))}", flush=True)

    # --- Prepare arguments for multiprocessing ---
    # Each worker gets (output_dir, cell_id, input_fastq_dir, args_namespace)
    mp_args_list = [(args.output_dir, cell_id, args.input_dir, args) for cell_id in sorted(list(cell_ids_to_process))]

    # --- Run processing in parallel ---
    if args.processes > 1 and len(mp_args_list) > 1:
        print(f"Starting parallel processing with {args.processes} workers.", flush=True)
        with multiprocessing.Pool(processes=args.processes) as pool:
            pool.map(helper_process_cell_pipeline, mp_args_list)
    elif mp_args_list: # Run sequentially if processes=1 or only one cell
        print("Starting sequential processing.", flush=True)
        for mp_args in mp_args_list:
            helper_process_cell_pipeline(mp_args)
    else:
        print("No tasks to run.", flush=True)


    print("--- Pipeline finished ---", flush=True)

